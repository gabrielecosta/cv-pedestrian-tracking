{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduzione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importazione delle librerie necessarie\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.font_manager as fm\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caricamento del modello DETR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caricamento del modello RESNET50\n",
    "model = torch.hub.load('facebookresearch/detr:main', 'detr_resnet50', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
    "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = [\n",
    "    'N/A', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A',\n",
    "    'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',\n",
    "    'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack',\n",
    "    'umbrella', 'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',\n",
    "    'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n",
    "    'skateboard', 'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass',\n",
    "    'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',\n",
    "    'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',\n",
    "    'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table', 'N/A',\n",
    "    'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n",
    "    'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A',\n",
    "    'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n",
    "    'toothbrush'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per convertire x, y, w e h in (x1, y1) e (x2, y2)\n",
    "def box_cxcywh_to_xyxy(x):\n",
    "    x_c, y_c, w, h = x.unbind(1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
    "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "\n",
    "    return torch.stack(b, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per plottare i risultati\n",
    "def plot_results(pil_img, prob, boxes):\n",
    "    plt.figure(figsize=(16,10))\n",
    "    plt.imshow(pil_img)\n",
    "    ax = plt.gca()\n",
    "    for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), COLORS * 100):\n",
    "        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                   fill=False, color=c, linewidth=3))\n",
    "        cl = p.argmax()\n",
    "        text = f'{CLASSES[cl]}: {p[cl]:0.2f}'\n",
    "        ax.text(xmin, ymin, text, fontsize=15,\n",
    "                bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per riscalare le bounding boxes\n",
    "def rescale_bboxes(boxes, size):\n",
    "    \n",
    "    img_w, img_h = size\n",
    "    b = box_cxcywh_to_xyxy(boxes)\n",
    "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
    "\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_w_detections(image, detections):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(image)\n",
    "    for detection in detections:\n",
    "        frame,id,x,y,w,h,conf,_,_,_ = detection\n",
    "        rectangle = patches.Rectangle((x, y), w, h, linewidth=1, edgecolor='green', facecolor='none')\n",
    "        ax.add_patch(rectangle)\n",
    "\n",
    "        # Step 4: Add text\n",
    "        # Define the text and its position\n",
    "        text = f\"id: {id}, conf:{conf:.2f}\"\n",
    "        text_position = (x, y-10)  # Position the text at the top-left corner with some padding\n",
    "        # Add the text to the plot with alignment properties\n",
    "        ax.text(*text_position, text, fontsize=5, color='green',\n",
    "        verticalalignment='top', horizontalalignment='left', bbox=dict(facecolor='white', alpha=0.5))\n",
    "\n",
    "\n",
    "    # Step 5: Display the image\n",
    "    plt.axis('off')  # Turn off the axis\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caricamento del modello VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sim_VGG16_net:\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Carica il modello VGG16 pre-addestrato\n",
    "        self.base_model = VGG16(weights='imagenet', include_top=True)\n",
    "        \n",
    "        # Estrai l'output dello strato prima dell'ultimo strato completamente connesso\n",
    "        self.model = Model(inputs=self.base_model.input, outputs=self.base_model.get_layer('fc2').output)\n",
    "\n",
    "    # Funzione per caricare e pre-processare un'immagine\n",
    "    def load_and_preprocess_image(self, frame):\n",
    "        \n",
    "        # img = image.load_img(image_path, target_size=(224, 224))\n",
    "        # VGG accetta in input immagini 224x224\n",
    "        frame = frame.convert('RGB')\n",
    "        img_resized = frame.resize((224, 224))\n",
    "        img_array = image.img_to_array(img_resized)\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        img_array = preprocess_input(img_array)\n",
    "        return img_array\n",
    "\n",
    "    # Funzione per estrarre le features da un'immagine\n",
    "    def extract_features_nb(self, frame):\n",
    "        \n",
    "        img = self.load_and_preprocess_image(frame)\n",
    "        features = self.model.predict(img)\n",
    "        return features.flatten()\n",
    "\n",
    "    # Funzione per estrarre le features da un'immagine\n",
    "    def extract_features(self, frame, bbox):\n",
    "        \n",
    "        # Definisci il bounding box per il crop\n",
    "        # (left, upper): The coordinates of the top-left corner of the bounding box.\n",
    "        # (right, lower): The coordinates of the bottom-right corner of the bounding box.\n",
    "        x1,y1,x2,y2 = bbox  # (left, upper, right, lower)\n",
    "        x1 = int(x1)\n",
    "        y1 = int(y1)\n",
    "        x2 = int(x2)\n",
    "        y2 = int(y2)\n",
    "        sub_box = (x1,y1,x2,y2)\n",
    "        subbox = frame.crop(sub_box)\n",
    "        img = self.load_and_preprocess_image(subbox)\n",
    "        \n",
    "        # per togliere il verbose model.predict(x,verbose=0)\n",
    "        # features = self.model.predict(img)\n",
    "        features = self.model.predict(img,verbose=0)\n",
    "        return features.flatten()\n",
    "\n",
    "    # Funzione per calcolare la similarit√† tra due immagini basata sulla distanza coseno delle features\n",
    "    def calculate_similarity(self, frame1, frame2):\n",
    "        features1 = self.extract_features(frame1)\n",
    "        features2 = self.extract_features(frame2)\n",
    "        # Calcola la distanza coseno tra le features\n",
    "        similarity = 1 - cosine(features1, features2)\n",
    "        return similarity\n",
    "\n",
    "    def calculate_similarity_reid(self, frame1, features2):\n",
    "        features1 = self.extract_features(frame1)\n",
    "        similarity = 1 - cosine(features1, features2)\n",
    "        return similarity\n",
    "\n",
    "    def calulate_similarity_features(self,features1, features2):\n",
    "        similarity = 1 - cosine(features1, features2)\n",
    "        return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Istanziamento di una VGG16\n",
    "vgg16 = sim_VGG16_net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tracker:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        # Lista di tracker\n",
    "        self.trackers = []\n",
    "\n",
    "        # Contatore per assegnare ID univoci ai pedoni\n",
    "        self.track_counter = 0\n",
    "\n",
    "        # Quanti frame devo aspettare prima che il tracker venga rimosso dall'immagine\n",
    "        self.max_lost_frames = 30\n",
    "\n",
    "        # Si definisce un vettore di vanishing tracks, il quale serve per determinare i track precedentemente persi.\n",
    "        \n",
    "        # Rispecchia la struttura di un tracker, ma √® formato da <id>, <feature_desc> e <lost>\n",
    "        self.vanishing_tracks = []\n",
    "\n",
    "        # Si definisce un vettore di track morti\n",
    "        self.dead_tracks = []\n",
    "\n",
    "    def update_tracker(self, confidences, detections, frame, vgg16, threshold_det_track=1.0, threshold_reid=1.0):\n",
    "\n",
    "        # Se non ci sono tracker, significa che √® il primo insieme di rilevamenti, quindi bisogna aggiungere ogni nuovo oggetto tracciato\n",
    "        if not self.trackers:\n",
    "            \n",
    "            # Aggiunta delle detection rilevate alla lista di tracker\n",
    "            for (detection, conf) in zip(detections, confidences):\n",
    "\n",
    "                track = {\"bbox\": detection, \"id\" : self.track_counter, \"conf\": conf, \"lost\": 0}\n",
    "                self.trackers.append(track)\n",
    "                self.track_counter += 1\n",
    "\n",
    "        # In caso contrario, bisogna gestire i rilevamenti esistenti\n",
    "        else:\n",
    "\n",
    "            # Memorizza i rilevamenti esistenti, ovvero le identit√† presenti al momento\n",
    "            current_bboxes = [tracker['bbox'] for tracker in self.trackers if tracker['lost'] == 0]\n",
    "            current_frames = [tracker for tracker in self.trackers if tracker['lost'] == 0]\n",
    "\n",
    "            # Calcolo della matrice di costo\n",
    "            cost_matrix = np.array([[self.compute_cost(tracker, detection) for detection in detections] for tracker in current_bboxes])\n",
    "\n",
    "            # Normalizzazione della matrice di costo\n",
    "            norm_cost_matrix = cost_matrix / cost_matrix.max()\n",
    "\n",
    "            # Applicazione dell'algoritmo per il bipartite matching (algoritmo ungherese)\n",
    "            row_indices, col_indices = linear_sum_assignment(norm_cost_matrix)\n",
    "\n",
    "            # Crea una lista di coppie con le corrispondenze ottimali\n",
    "            matched_indices = list(zip(row_indices, col_indices))\n",
    "\n",
    "            # Crea dei set di detection e tracker non matchati\n",
    "            unmatched_detections = set(range(len(detections))) - set(col_indices)\n",
    "            unmatched_trackers = set(range(len(current_bboxes))) - set(row_indices)\n",
    "\n",
    "\n",
    "            # Iterazione su tutte le coppie di indici corrispondenti ottenute dall'algoritmo ungherese.\n",
    "            # Per ogni coppia aggiorna il bounding box e rimposta il contatore di fotogrammi persi a zero perch√© il rilevamento dell'oggetto continua.\n",
    "            \n",
    "            for t_idx, d_idx in matched_indices:\n",
    "                \n",
    "                # qui bisogna mettere una soglia sulle assegnazioni corrispondenti,\n",
    "                # se il valore della matrice di costo C[t_idx, d_idx] √® maggiore di un certo valore allora assegna\n",
    "                # altrimenti √® un lost!\n",
    "                # usare theshold_det_track\n",
    "                # print(norm_cost_matrix[t_idx, d_idx])\n",
    "                # se stanno sotto la soglia allora vanno bene, altrimenti devo scartarli\n",
    "                # sotto perch√© √® un problema di minimo\n",
    "                \n",
    "                id_track = current_frames[t_idx]['id']\n",
    "                \n",
    "                if self.trackers[id_track]['lost'] == 0:\n",
    "                    \n",
    "                    # questo vale solo per le non lost detections\n",
    "                    if norm_cost_matrix[t_idx, d_idx] <= threshold_det_track:\n",
    "                        print(f\"Tracker con {id_track} identificato tra i frame.\")\n",
    "                        self.trackers[id_track]['bbox'] = detections[d_idx] # aggiorna la bounding box\n",
    "                        self.trackers[id_track]['conf'] = confidences[d_idx] # aggiorna la confidence\n",
    "                        self.trackers[id_track]['lost'] = 0 # aggiorna il numero di frame persi\n",
    "\n",
    "                    else:\n",
    "                        \n",
    "                        self.trackers[id_track]['lost'] += 1\n",
    "                        \n",
    "                        bbx = self.trackers[id_track]['bbox']\n",
    "                        \n",
    "                        feature_lost = vgg16.extract_features(frame,bbx)\n",
    "                        \n",
    "                        lost_track = {'id':id_track, 'bbox': self.trackers[id_track]['bbox'], 'conf': self.trackers[id_track]['conf'], 'feature': feature_lost}\n",
    "                        \n",
    "                        self.vanishing_tracks.append(lost_track)\n",
    "\n",
    "            # Aggiungi nuovi rilevamenti che non hanno corrispondenze precedenti alla lista dei tracker\n",
    "            remaining_detection = [detections[d_idx] for d_idx in unmatched_detections]\n",
    "            remaining_confidences = [confidences[d_idx] for d_idx in unmatched_detections]\n",
    "\n",
    "            if len(self.vanishing_tracks) != 0 and len(unmatched_detections) != 0:\n",
    "\n",
    "                vanishing_features = [vanishing['feature'] for vanishing in self.vanishing_tracks]\n",
    "                remaining_features = [vgg16.extract_features(frame, det) for det in remaining_detection]\n",
    "                sim_matrix = np.array([[vgg16.calulate_similarity_features(vanishing_feature, remaining_feature) for remaining_feature in remaining_features ] for vanishing_feature in vanishing_features])\n",
    "                IoU_matrix = np.array([[self.iou(van['bbox'], det) for det in remaining_detection] for van in self.vanishing_tracks])\n",
    "                norm_cost_matrix_reid = 0.6 * (1 - sim_matrix) + 0.4 * (1 - IoU_matrix)\n",
    "\n",
    "                row_indices, col_indices = linear_sum_assignment(norm_cost_matrix_reid)\n",
    "\n",
    "                # Crea una lista di coppie con le corrispondenze ottimali\n",
    "                matched_indices_reid = list(zip(row_indices, col_indices)) # ho matchato le vanishing\n",
    "\n",
    "                vanishing_list_enumerate = []\n",
    "                \n",
    "                for t_idx, det in enumerate(self.vanishing_tracks):\n",
    "                    vanishing_list_enumerate.append(det)\n",
    "\n",
    "                # riassegno le matched solo se hanno un valore di soglia opportuno\n",
    "                for t_idx, d_idx in matched_indices_reid:\n",
    "                    \n",
    "                    feature_1_test = vgg16.extract_features(frame,remaining_detection[d_idx])\n",
    "                    feature_2_test = vanishing_list_enumerate[t_idx]['feature']\n",
    "                    \n",
    "                    print(f'Similarit√†: {vgg16.calulate_similarity_features(feature_1_test,feature_2_test)}')\n",
    "                    \n",
    "                    bbox = vanishing_list_enumerate[t_idx]['bbox']\n",
    "                    \n",
    "                    # print(f'IoU sim: {self.iou(bbox,remaining_detection[d_idx])}')\n",
    "                    if norm_cost_matrix_reid[t_idx, d_idx] <= threshold_reid:\n",
    "                        \n",
    "                        # aggiorno utilizzando l'ID delle vanished che cammina di pari passo con trackers\n",
    "                        # con le remaining_detections\n",
    "                        print(f\"Effettuata reidentificazione: {vanishing_list_enumerate[t_idx]['id']}\")\n",
    "                        self.trackers[vanishing_list_enumerate[t_idx]['id']]['bbox'] = remaining_detection[d_idx] # aggiorna la bounding box\n",
    "                        self.trackers[vanishing_list_enumerate[t_idx]['id']]['conf'] = remaining_confidences[d_idx] # aggiorna la confidence\n",
    "                        self.trackers[vanishing_list_enumerate[t_idx]['id']]['lost'] = 0 # aggiorna il numero di frame persi\n",
    "                        # tolgo da vanishing\n",
    "                        self.vanishing_tracks = [t for t in self.vanishing_tracks if t['id'] != vanishing_list_enumerate[t_idx]['id']]\n",
    "                    \n",
    "                    else:\n",
    "                        \n",
    "                        print(f\"Valore sopra la soglia per la re-id: {norm_cost_matrix_reid[t_idx, d_idx]}\")\n",
    "                        \n",
    "                        # se non reidentifico allora devo creare una nuova detection\n",
    "                        new_track = {'bbox': remaining_detection[d_idx], 'id': self.track_counter, 'conf': remaining_confidences[d_idx], 'lost':0}\n",
    "                        self.trackers.append(new_track)\n",
    "                        self.track_counter += 1\n",
    "\n",
    "                # Crea dei set di detections non matchate con le vanishing, quindi nuove detections\n",
    "                unmatched_unmatched_detections = set(range(len(remaining_detection))) - set(col_indices)\n",
    "\n",
    "                # adesso provo a matchare e unmatchare con i vanishing\n",
    "                # se matchano allora provvedo a reinserire nel tracker l'id a lost=0,\n",
    "                # aggiorno la confidence e la boundary box sulla base della matchata\n",
    "                # altrimenti la devo assegnae nuova\n",
    "                \n",
    "                for d_idx in unmatched_unmatched_detections:\n",
    "                    print(\"New detection after re-id not found!\")\n",
    "                    new_track = {'bbox': remaining_detection[d_idx], 'id': self.track_counter, 'conf': remaining_confidences[d_idx], 'lost':0}\n",
    "                    self.trackers.append(new_track)\n",
    "                    self.track_counter += 1\n",
    "            else:\n",
    "                # ancora non ci sono track scomparse\n",
    "                for d_idx in unmatched_detections:\n",
    "                    \n",
    "                    print(\"New detection!\")\n",
    "                    new_track = {'bbox': detections[d_idx], 'id': self.track_counter, 'conf':confidences[d_idx], 'lost':0}\n",
    "                    self.trackers.append(new_track)\n",
    "                    self.track_counter += 1\n",
    "\n",
    "            vanished_keys = [vanished['id'] for vanished in self.vanishing_tracks]\n",
    "\n",
    "            # devo aggiornare i track persi di quelli gi√† persi\n",
    "            for lost_vanished_key in vanished_keys:\n",
    "                if self.trackers[lost_vanished_key]['lost'] > 0:\n",
    "                    self.trackers[lost_vanished_key]['lost'] += 1\n",
    "\n",
    "            # Aumenta il contatore per i tracker persi al frame corrente!\n",
    "            # stampo gli unmatched track\n",
    "            for t_idx in unmatched_trackers:\n",
    "\n",
    "                id_track = current_frames[t_idx]['id']\n",
    "                print(f\"Lost track id: {id_track}\")\n",
    "                self.trackers[id_track]['lost'] += 1\n",
    "                bbx = self.trackers[id_track]['bbox']\n",
    "                print(f'Track gi√† scomparsa? {id_track not in vanished_keys}')\n",
    "                print(f'Track scomparse: {self.vanishing_tracks}')\n",
    "                if id_track not in vanished_keys:\n",
    "                    feature_lost = vgg16.extract_features(frame,bbx)\n",
    "                    lost_track = {'id':id_track, 'bbox': self.trackers[id_track]['bbox'], 'conf': self.trackers[id_track]['conf'], 'feature': feature_lost}\n",
    "                    self.vanishing_tracks.append(lost_track)\n",
    "\n",
    "\n",
    "            keep_tracks = []\n",
    "            # le riaggiorno\n",
    "            vanished_keys = [vanished['id'] for vanished in self.vanishing_tracks]\n",
    "\n",
    "            for lost_vanished_key in vanished_keys:\n",
    "                \n",
    "                if self.trackers[lost_vanished_key]['lost'] > self.max_lost_frames:\n",
    "                    self.dead_tracks.append(self.trackers[lost_vanished_key]['id'])\n",
    "                    id_dead = self.trackers[lost_vanished_key]['id']\n",
    "                    print(f'Morta la track {id_dead}')\n",
    "                \n",
    "                if self.trackers[lost_vanished_key]['lost'] > 0 and self.trackers[lost_vanished_key]['lost'] <= self.max_lost_frames:\n",
    "                    keep_tracks.append(self.trackers[lost_vanished_key]['id'])\n",
    "\n",
    "            # tolgo da vanishing le track perse ma non ancora morte\n",
    "            self.vanishing_tracks = [t for t in self.vanishing_tracks if t['id'] in keep_tracks]\n",
    "            print(f'Track perse conservate: {self.vanishing_tracks}')\n",
    "\n",
    "\n",
    "    def compute_cost(self, tracker, detection):\n",
    "        t_x1, t_y1, t_x2, t_y2 = tracker\n",
    "        d_x1, d_y1, d_x2, d_y2 = detection\n",
    "        iou = self.iou(tracker, detection)\n",
    "        # dist = np.linalg.norm(np.array([(t_x1+t_x2)/2, (t_y1+t_y2)/2]) - np.array([(d_x1+d_x2)/2, (d_y1+d_y2)/2]))\n",
    "        return (1-iou) # la distanza varia tra 0 e 1\n",
    "\n",
    "    def iou(self, box1, box2):\n",
    "        x1 = max(box1[0], box2[0])\n",
    "        y1 = max(box1[1], box2[1])\n",
    "        x2 = min(box1[2], box2[2])\n",
    "        y2 = min(box1[3], box2[3])\n",
    "        inter_area = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "        box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "        box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "        union_area = box1_area + box2_area - inter_area\n",
    "        return inter_area / union_area if union_area != 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preload detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_file = \"outputs/MOT17-02-DPM/MOT17-02-DPM-DETR06.txt\"\n",
    "\n",
    "data_preloaded = []\n",
    "\n",
    "with open(sequence_file, \"r\") as f:\n",
    "\n",
    "    for row in f:\n",
    "\n",
    "        frame, x1, x2, y1, y2, confidence = row.split(\",\")\n",
    "\n",
    "        frame = int(frame)\n",
    "        x1 = float(x1)\n",
    "        x2 = float(x2)\n",
    "        y1 = float(y1)\n",
    "        y2 = float(y2)\n",
    "\n",
    "        conf = float(confidence)\n",
    "\n",
    "        detection_frame = [frame, [x1, x2, y1, y2], conf]\n",
    "        data_preloaded.append(detection_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_detections = {}\n",
    "for data in data_preloaded:\n",
    "    if data[0] not in final_detections.keys():\n",
    "        final_detections[data[0]] = []\n",
    "    final_detections[data[0]].append([data[1], data[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detections_frame_1 = final_detections[1]\n",
    "detections_frame_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for detection, confidence in final_detections[1]:\n",
    "    print(f'Detection: {detection}; Confidence: {confidence}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elaborazione del video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image_folder_preloaded(detections_loaded, folder_path, frame_size=(640, 360), detection_interval=1, frame_limit_flag = False, limit=5, threshold_det_track=0.4, threshold_reid=0.4):\n",
    "    tracker = Tracker()\n",
    "    frame_files = sorted([os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "    frame_count = 0\n",
    "\n",
    "    detections_frame = []\n",
    "\n",
    "    for frame_file in frame_files:\n",
    "        # detr_preloaded = []\n",
    "\n",
    "        if frame_limit_flag and frame_count > limit:\n",
    "            break\n",
    "        frame = Image.open(frame_file)\n",
    "\n",
    "        if frame_count % detection_interval == 0:\n",
    "            # confidences, detections = detect_pedestrians(im=frame, model=model)\n",
    "            detr_preloaded = detections_loaded[frame_count]\n",
    "        detections_detr = []\n",
    "        confidences_detr = []\n",
    "        for detr in detr_preloaded:\n",
    "            detections_detr.append(detr[0])\n",
    "            confidences_detr.append(detr[1])\n",
    "        # print(detections_detr)\n",
    "        # quindi da qua in poi non cambia pi√π nulla rispetto a prima\n",
    "        tracker.update_tracker(confidences_detr, detections_detr, frame, vgg16, threshold_det_track, threshold_reid)\n",
    "\n",
    "        actual_detections = [] # solo per print\n",
    "\n",
    "        for track in tracker.trackers:\n",
    "            if track['lost'] == 0:\n",
    "                x1, y1, x2, y2 = map(int, track['bbox'])\n",
    "                x = x1\n",
    "                y = y1\n",
    "                w = x2-x1\n",
    "                h = y2-y1\n",
    "                conf = track['conf']\n",
    "                # poi format_detection deve essere stampato in un file\n",
    "                format_detectetion = [frame_count, track['id'], x,y,w,h, track['conf'],-1,-1,-1]\n",
    "                print(format_detectetion)\n",
    "                actual_detections.append(format_detectetion) # solo per printing\n",
    "                detections_frame.append(format_detectetion)\n",
    "        print(f'Frame: {frame_count}')\n",
    "        plot_image_w_detections(frame, actual_detections)\n",
    "        frame_count += 1\n",
    "    return detections_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prova sul campo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images path\n",
    "images_path = f\"dataset/train/MOT17-02-DPM/img1\"\n",
    "\n",
    "# soglia matching = 0.4, soglia re_id = 0.4\n",
    "detections_to_save_0404 = process_image_folder_preloaded(final_detections, images_path, frame_size=(1920, 1080), detection_interval=1, frame_limit_flag=False, limit=10, threshold_det_track=0.4, threshold_reid=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save path\n",
    "save_path = f\"outputs/MOT17-02-DPM.txt\"\n",
    "\n",
    "with open(save_path, 'w') as f:\n",
    "    \n",
    "    for d in detections_to_save_0404:\n",
    "        \n",
    "        frame,id,x,y,w,h,conf,_,_,_ = d\n",
    "        \n",
    "        print(f'{frame+1},{id},{x},{y},{w},{h},{conf},-1,-1,-1', file=f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
