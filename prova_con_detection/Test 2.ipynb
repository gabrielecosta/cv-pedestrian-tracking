{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.optimize import linear_sum_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/gabriele/.cache/torch/hub/facebookresearch_detr_main\n",
      "/home/gabriele/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/gabriele/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('facebookresearch/detr:main', 'detr_resnet50', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = [\n",
    "    'N/A', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A',\n",
    "    'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',\n",
    "    'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack',\n",
    "    'umbrella', 'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',\n",
    "    'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n",
    "    'skateboard', 'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass',\n",
    "    'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',\n",
    "    'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',\n",
    "    'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table', 'N/A',\n",
    "    'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n",
    "    'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A',\n",
    "    'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n",
    "    'toothbrush'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
    "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def detect(model, im, transform = None, threshold_confidence = 0.7):\\n    if transform is None:\\n        # standard PyTorch mean-std input image normalization\\n        transform = T.Compose([\\n        T.Resize(800),\\n        T.ToTensor(),\\n        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\\n        ])\\n\\n    img = transform(im).unsqueeze(0)\\n\\n    # demo model only support by default images with aspect ratio between 0.5 and 2\\n    # if you want to use images with an aspect ratio outside this range\\n    # rescale your image so that the maximum size is at most 1333 for best results\\n    assert img.shape[-2] <= 1600 and img.shape[-1] <= 1600, 'demo model only supports images up to 1600 pixels on each side'\\n\\n    # propagate through the model\\n    outputs = model(img)\\n\\n    # keep only predictions with a confidence > threshold_confidence\\n    probas = outputs['pred_logits'].softmax(-1)[0, :, :-1]\\n    keep = probas.max(-1).values > threshold_confidence\\n\\n    # convert boxes from [0; 1] to image scales\\n    bboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0, keep], im.size)\\n    return probas[keep], bboxes_scaled\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def box_cxcywh_to_xyxy(x):\n",
    "    x_c, y_c, w, h = x.unbind(1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
    "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "\n",
    "    return torch.stack(b, dim=1)\n",
    "\n",
    "'''def rescale_bboxes(out_bbox, size):\n",
    "    img_w, img_h, _ = size\n",
    "    b = box_cxcywh_to_xyxy(out_bbox)\n",
    "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
    "    return b'''\n",
    "\n",
    "def plot_results(pil_img, prob, boxes):\n",
    "    plt.figure(figsize=(16,10))\n",
    "    plt.imshow(pil_img)\n",
    "    ax = plt.gca()\n",
    "    for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), COLORS * 100):\n",
    "        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                   fill=False, color=c, linewidth=3))\n",
    "        cl = p.argmax()\n",
    "        text = f'{CLASSES[cl]}: {p[cl]:0.2f}'\n",
    "        ax.text(xmin, ymin, text, fontsize=15,\n",
    "                bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "'''def detect(model, im, transform = None, threshold_confidence = 0.7):\n",
    "    if transform is None:\n",
    "        # standard PyTorch mean-std input image normalization\n",
    "        transform = T.Compose([\n",
    "        T.Resize(800),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    img = transform(im).unsqueeze(0)\n",
    "\n",
    "    # demo model only support by default images with aspect ratio between 0.5 and 2\n",
    "    # if you want to use images with an aspect ratio outside this range\n",
    "    # rescale your image so that the maximum size is at most 1333 for best results\n",
    "    assert img.shape[-2] <= 1600 and img.shape[-1] <= 1600, 'demo model only supports images up to 1600 pixels on each side'\n",
    "\n",
    "    # propagate through the model\n",
    "    outputs = model(img)\n",
    "\n",
    "    # keep only predictions with a confidence > threshold_confidence\n",
    "    probas = outputs['pred_logits'].softmax(-1)[0, :, :-1]\n",
    "    keep = probas.max(-1).values > threshold_confidence\n",
    "\n",
    "    # convert boxes from [0; 1] to image scales\n",
    "    bboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0, keep], im.size)\n",
    "    return probas[keep], bboxes_scaled'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_bboxes(boxes, size):\n",
    "\n",
    "    img_w, img_h = size\n",
    "\n",
    "    b = box_cxcywh_to_xyxy(boxes)\n",
    "\n",
    "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
    "    \n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per rilevare i pedoni (modifica di quella della prof)\n",
    "def detect_pedestrians(model, im, transform = None, threshold_confidence = 0.7):\n",
    "    if transform is None:\n",
    "\n",
    "        # standard PyTorch mean-std input image normalization\n",
    "        transform = T.Compose([\n",
    "        T.Resize(800),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    img = transform(im).unsqueeze(0)\n",
    "\n",
    "    # demo model only support by default images with aspect ratio between 0.5 and 2\n",
    "    # if you want to use images with an aspect ratio outside this range\n",
    "    # rescale your image so that the maximum size is at most 1333 for best results\n",
    "    assert img.shape[-2] <= 1600 and img.shape[-1] <= 1600, 'demo model only supports images up to 1600 pixels on each side'\n",
    "\n",
    "    outputs = model(img)\n",
    "\n",
    "    # keep only predictions with a confidence > threshold_confidence\n",
    "    probas = outputs['pred_logits'].softmax(-1)[0, :, :-1]\n",
    "    max_probas = probas.max(-1).values\n",
    "    keep = probas.max(-1).values > threshold_confidence\n",
    "    labels = probas.argmax(-1)\n",
    "\n",
    "    # Filter by pedestrian\n",
    "    keep = keep & (labels == 1)\n",
    "\n",
    "    # Extract the confidences for the kept boxes\n",
    "    confidences = max_probas[keep].detach().numpy()\n",
    "\n",
    "    # convert boxes from [0; 1] to image scales\n",
    "    bboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0, keep], im.size)\n",
    "    return confidences, bboxes_scaled.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8462946  0.99900097]\n",
      "[[545.5866  255.92393 552.98315 272.4574 ]\n",
      " [ 56.18371  75.05348 287.72366 421.8405 ]]\n"
     ]
    }
   ],
   "source": [
    "conf, bboxes_scaled = detect_pedestrians(model, Image.open('im.jpg'))\n",
    "print(conf)\n",
    "print(bboxes_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tracker:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        # Lista di tracker\n",
    "        self.trackers = []\n",
    "\n",
    "        # Contatore per assegnare ID univoci ai pedoni\n",
    "        self.track_counter = 0\n",
    "\n",
    "        # Quanti frame devo aspettare prima che il tracker venga rimosso dall'immagine\n",
    "        self.max_lost_frames = 30\n",
    "\n",
    "    def update_tracker(self, confidences, detections):\n",
    "\n",
    "        # Se non ci sono tracker, significa che è il primo insieme di rilevamenti, quindi bisogna aggiungere ogni nuovo oggetto tracciato\n",
    "        if not self.trackers:\n",
    "            for (detection,conf) in (detections,confidences):\n",
    "                track = {'bbox': detection, 'id': self.track_counter, 'conf':conf, 'lost':0, }\n",
    "                self.trackers.append(track)\n",
    "                self.track_counter += 1\n",
    "        \n",
    "        # In caso contrario, bisogna gestire i rilevamenti esistenti\n",
    "        else:\n",
    "\n",
    "            # Memorizza i rilevamenti esistenti\n",
    "            current_bboxes = [tracker['bbox'] for tracker in self.trackers]\n",
    "\n",
    "            # Calcola la matrice di costo\n",
    "            cost_matrix = np.zeros((len(current_bboxes), len(detections)))\n",
    "\n",
    "            for tracker_index, tracker in enumerate(current_bboxes):\n",
    "                for detection_index, detection in enumerate(detections):\n",
    "                    cost_matrix[tracker_index, detection_index] = self.compute_cost(tracker, detection)\n",
    "            \n",
    "            row_indices, col_indices = linear_sum_assignment(cost_matrix)\n",
    "\n",
    "            # Crea una lista di coppie con le corrispondenze ottimali\n",
    "            matched_indices = list(zip(row_indices, col_indices))\n",
    "\n",
    "            # Crea dei set di detection e tracker non matchati\n",
    "            unmatched_detections = set(range(len(detections))) - set(col_indices)\n",
    "            unmatched_trackers = set(range(len(current_bboxes))) - set(row_indices)\n",
    "\n",
    "            # Itera su tutte le coppie di indici corrispondenti ottenute dall'algoritmo ungherese.\n",
    "            # Per ogni coppia aggiorna il bounding box e rimposta il contatore di fotogrammi persi a zero perché il rilevamento dell'oggetto continua.\n",
    "            for t_idx, d_idx in matched_indices:\n",
    "                self.trackers[t_idx]['bbox'] = detections[d_idx]\n",
    "                self.trackers[t_idx]['lost'] = 0\n",
    "\n",
    "            # Aggiungi nuovi rilevamenti che non hanno corrispondenze precedenti alla lista dei tracker\n",
    "            for d_idx in unmatched_detections:\n",
    "                new_track = {'bbox': detections[d_idx], 'id': self.track_counter, 'conf':conf, 'lost':0}\n",
    "                self.trackers.append(new_track)\n",
    "                self.track_counter += 1\n",
    "\n",
    "            # Aumenta il contatore per i tracker persi\n",
    "            for t_idx in unmatched_trackers:\n",
    "                self.trackers[t_idx]['lost'] += 1\n",
    "\n",
    "            # Rimuovi gli oggetti non tracciati per troppo tempo\n",
    "            self.trackers = [t for t in self.trackers if t['lost'] <= self.max_lost_frames]\n",
    "    \n",
    "    def compute_cost(self, tracker, detection):\n",
    "        t_x1, t_y1, t_x2, t_y2 = tracker\n",
    "        d_x1, d_y1, d_x2, d_y2 = detection\n",
    "        iou = self.iou(tracker, detection)\n",
    "        dist = np.linalg.norm(np.array([(t_x1+t_x2)/2, (t_y1+t_y2)/2]) - np.array([(d_x1+d_x2)/2, (d_y1+d_y2)/2]))\n",
    "        return 1 - iou + 0.5 * dist\n",
    "\n",
    "    def iou(self, box1, box2):\n",
    "        x1 = max(box1[0], box2[0])\n",
    "        y1 = max(box1[1], box2[1])\n",
    "        x2 = min(box1[2], box2[2])\n",
    "        y2 = min(box1[3], box2[3])\n",
    "        inter_area = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "        box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "        box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "        union_area = box1_area + box2_area - inter_area\n",
    "        return inter_area / union_area if union_area != 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image_folder(folder_path, frame_size=(640, 360), detection_interval=1, frame_limit_flag = False, limit=5):\n",
    "    tracker = Tracker()\n",
    "    frame_files = sorted([os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "    frame_count = 0\n",
    "    \n",
    "    for frame_file in frame_files:\n",
    "        if frame_limit_flag and frame_count > limit:\n",
    "            break\n",
    "        frame = Image.open(frame_file)\n",
    "        #frame = cv2.imread(frame_file)\n",
    "        #frame = cv2.resize(frame, frame_size)\n",
    "        \n",
    "        if frame_count % detection_interval == 0:\n",
    "            confidences, detections = detect_pedestrians(im=frame, model=model)\n",
    "        tracker.update_tracker(confidences, detections)\n",
    "        \n",
    "        for track in tracker.trackers:\n",
    "            x1, y1, x2, y2 = map(int, track['bbox'])\n",
    "\n",
    "            # print(track)\n",
    "\n",
    "            #cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            #cv2.putText(frame, f'ID: {track[\"id\"]}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        \n",
    "        #cv2.imshow('Multi-Object Tracking', frame)\n",
    "        #if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        #    break\n",
    "        print(frame_count)\n",
    "        frame_count += 1\n",
    "    \n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'image_path =\"/home/ivan/Unipa/Materie/Visione artificiale/Assignments/Assignment 3/dataset/test/MOT17-01-DPM/img1/000001.jpg\"\\nmodel = torch.hub.load(\\'facebookresearch/detr:main\\', \\'detr_resnet50\\', pretrained=True)\\nimg = Image.open(image_path)\\nprob, bboxes_scaled = detect_pedestrians(model, img)\\nplot_results(img, prob, bboxes_scaled)'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''image_path =\"/home/ivan/Unipa/Materie/Visione artificiale/Assignments/Assignment 3/dataset/test/MOT17-01-DPM/img1/000001.jpg\"\n",
    "model = torch.hub.load('facebookresearch/detr:main', 'detr_resnet50', pretrained=True)\n",
    "img = Image.open(image_path)\n",
    "prob, bboxes_scaled = detect_pedestrians(model, img)\n",
    "plot_results(img, prob, bboxes_scaled)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process_video('test.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprocess_image_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../MOT17/test/MOT17-08-FRCNN/img1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m640\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m360\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetection_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_limit_flag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[28], line 15\u001b[0m, in \u001b[0;36mprocess_image_folder\u001b[0;34m(folder_path, frame_size, detection_interval, frame_limit_flag, limit)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m frame_count \u001b[38;5;241m%\u001b[39m detection_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     14\u001b[0m     confidences, detections \u001b[38;5;241m=\u001b[39m detect_pedestrians(im\u001b[38;5;241m=\u001b[39mframe, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m---> 15\u001b[0m \u001b[43mtracker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_tracker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfidences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetections\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m track \u001b[38;5;129;01min\u001b[39;00m tracker\u001b[38;5;241m.\u001b[39mtrackers:\n\u001b[1;32m     18\u001b[0m     x1, y1, x2, y2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mint\u001b[39m, track[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbbox\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[27], line 18\u001b[0m, in \u001b[0;36mTracker.update_tracker\u001b[0;34m(self, confidences, detections)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_tracker\u001b[39m(\u001b[38;5;28mself\u001b[39m, confidences, detections):\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Se non ci sono tracker, significa che è il primo insieme di rilevamenti, quindi bisogna aggiungere ogni nuovo oggetto tracciato\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrackers:\n\u001b[0;32m---> 18\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m (detection,conf) \u001b[38;5;129;01min\u001b[39;00m (detections,confidences):\n\u001b[1;32m     19\u001b[0m             track \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbbox\u001b[39m\u001b[38;5;124m'\u001b[39m: detection, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrack_counter, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconf\u001b[39m\u001b[38;5;124m'\u001b[39m:conf, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlost\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m0\u001b[39m, }\n\u001b[1;32m     20\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrackers\u001b[38;5;241m.\u001b[39mappend(track)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "process_image_folder('../MOT17/test/MOT17-08-FRCNN/img1', frame_size=(640, 360), detection_interval=1, frame_limit_flag=True, limit=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
